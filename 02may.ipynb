{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "1:\n Anomaly detection is a technique used to identify unusual patterns or data points that do not\nconform to the expected behavior in a dataset. Its purpose is to detect outliers or anomalies that\nmay be indicative of errors, fraud, or other unusual events. Anomaly detection can be applied in\nvarious domains such as network intrusion detection, fraud detection, system health monitoring, and \nmany more. There are various methods of anomaly detection, including statistical methods, machine learning,\nand deep learning techniques. The choice of method depends on the type of data, the problem domain, and the specific use case.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "2:\n  Anomaly detection poses several challenges, including the lack of labeled data, high false-positive \nrates, and the need for real-time detection. Anomaly detection algorithms also need to be able to handle \nhigh-dimensional data and be scalable to large datasets. In addition, the choice of algorithm and parameters \ndepends on the specific use case and domain, which can be a challenge.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "3:\n Unsupervised anomaly detection differs from supervised anomaly detection in that unsupervised methods do not\nrequire labeled data for training. Instead, unsupervised methods use statistical or machine learning techniques\nto identify data points that are significantly different from the majority of the data. In contrast, supervised \nanomaly detection requires labeled data for training and involves building a model that can accurately classify\ndata points as either normal or anomalous based on the labeled data.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "4:\n  The main categories of anomaly detection algorithms include statistical methods, machine learning-based\nmethods, and deep learning-based methods. Statistical methods include techniques such as Z-score, Gaussian mixture models,\nand kernel density estimation. Machine learning-based methods involve training models to identify anomalies based on labeled\nor unlabeled data, such as k-nearest neighbors, one-class SVM, and isolation forests. Deep learning-based methods use neural \nnetworks to identify anomalies based on patterns and relationships in the data, such as autoencoders and variational autoencoders. \nThe choice of algorithm depends on the specific use case and the type of data being analyzed.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "5:\n Distance-based anomaly detection methods assume that normal data points are clustered together and \nare far from anomalous data points. They also assume that anomalous data points are isolated from the\nmajority of the data points.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "6:\n The LOF (Local Outlier Factor) algorithm computes anomaly scores based on the density of the local neighborhood around a data\npoint. It computes the local reachability density of each data point by comparing the average distance of its k-nearest neighbors \nto its own distance. The LOF score of a data point is the ratio of the average local reachability density of its k-nearest neighbors\nto its own local reachability density.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "7:\n    The key parameters of the Isolation Forest algorithm are the number of trees to be built and the subsampling size.\nThe number of trees determines the number of partitions made on the data, while the subsampling size determines the size\nof the random subsets of the data used to build the trees. A larger number of trees leads to a more finely partitioned space,\nwhile a larger subsampling size reduces the likelihood of overfitting.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "8:\n Assuming the data point in question has 10 neighbors within a radius of 0.5, and 2 of those neighbors belong to the same class as the data point, the anomaly score can be calculated as follows:\n\nAnomaly score = (number of neighbors with different class) / K\nAnomaly score = (10 - 2) / 10\nAnomaly score = 0.8\n\nTherefore, the anomaly score for the data point in question using KNN with K=10 is 0.8. This indicates that the data point is not very anomalous as it has a majority of neighbors belonging to the same class.   ",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "9:\n The anomaly score of a data point in Isolation Forest is calculated as the average path length of the data point across all trees. \nIn this case, since the dataset has 3000 data points and 100 trees, the average path length of the trees would be approximately 8.0. \nIf a data point has an average path length of 5.0, it means that it has a shorter average path length than most of the other data points\nin the dataset, making it a potential anomaly. Therefore, the anomaly score of this data point would be lower than the average anomaly \nscore of the dataset.",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}